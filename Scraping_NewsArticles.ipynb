{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1103732a-b99a-411f-b54d-460ad4880acc",
   "metadata": {},
   "source": [
    "Scraping from News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9aaa914-4a73-4355-a2b3-d85830796fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc06c685-403a-4adf-8e6a-abf06a07fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressScraper:\n",
    "    def __init__(self):\n",
    "        self.id = 0\n",
    "\n",
    "    def get_express_articles(self, max_pages=9):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": []\n",
    "        }\n",
    "        base_url = \"https://www.express.pk/sports\"\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url)\n",
    "                    response.raise_for_status()\n",
    "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                    # Finding article cards\n",
    "                    cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')\n",
    "                    print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                    success_count = 0\n",
    "                    for card in cards:\n",
    "                        try:\n",
    "                            div = card.find(\"div\", class_='horiz-news3-caption')\n",
    "\n",
    "                            # Article Title\n",
    "                            headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "                            # Article Link\n",
    "                            link = div.find('a')['href']\n",
    "                            \n",
    "                            # Requesting the content from each article's link\n",
    "                            article_response = requests.get(link)\n",
    "                            article_response.raise_for_status()\n",
    "                            content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                            # Content arranged in paras inside <span> tags\n",
    "                            paras = content_soup.find('span', class_='story-text').find_all('p')\n",
    "                            combined_text = \" \".join([p.get_text(strip=True).replace('\\xa0', ' ') for p in paras])\n",
    "\n",
    "                            # Storing data\n",
    "                            express_df['id'].append(self.id)\n",
    "                            express_df['title'].append(headline)\n",
    "                            express_df['link'].append(link)\n",
    "                            express_df['gold_label'].append(category.replace('saqafat', 'entertainment').replace('science', 'science-technology'))\n",
    "                            express_df['content'].append(combined_text)\n",
    "\n",
    "                            self.id += 1\n",
    "                            success_count += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                    print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "                    print(\"*\" * 50)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load page {page} of category '{category}': {e}\")\n",
    "\n",
    "        return pd.DataFrame(express_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a2557a-ca07-4dea-a982-9584e71a8adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 1 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 2 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 2 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 3 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 3 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 4 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 4 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 5 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 5 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 6 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 6 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 7 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 7 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 8 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 8 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 9 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 9 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 10 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 10 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'saqafat'.\n",
      "**************************************************\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 10 articles on page 1 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 10 articles on page 2 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 10 articles on page 3 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 10 articles on page 4 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 10 articles on page 5 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 10 articles on page 6 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 10 articles on page 7 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 8 of category 'business'...\n",
      "\t--> Found 10 articles on page 8 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 9 of category 'business'...\n",
      "\t--> Found 10 articles on page 9 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 10 of category 'business'...\n",
      "\t--> Found 10 articles on page 10 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'business'.\n",
      "**************************************************\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 10 articles on page 1 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 10 articles on page 2 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 10 articles on page 3 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 10 articles on page 4 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 10 articles on page 5 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 10 articles on page 6 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 10 articles on page 7 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 8 of category 'sports'...\n",
      "\t--> Found 10 articles on page 8 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 9 of category 'sports'...\n",
      "\t--> Found 10 articles on page 9 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 10 of category 'sports'...\n",
      "\t--> Found 10 articles on page 10 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'sports'.\n",
      "**************************************************\n",
      "Scraping page 1 of category 'science'...\n",
      "\t--> Found 10 articles on page 1 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 2 of category 'science'...\n",
      "\t--> Found 10 articles on page 2 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 3 of category 'science'...\n",
      "\t--> Found 10 articles on page 3 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 4 of category 'science'...\n",
      "\t--> Found 10 articles on page 4 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 5 of category 'science'...\n",
      "\t--> Found 10 articles on page 5 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 6 of category 'science'...\n",
      "\t--> Found 10 articles on page 6 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 7 of category 'science'...\n",
      "\t--> Found 10 articles on page 7 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 8 of category 'science'...\n",
      "\t--> Found 10 articles on page 8 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 9 of category 'science'...\n",
      "\t--> Found 10 articles on page 9 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 10 of category 'science'...\n",
      "\t--> Found 10 articles on page 10 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'science'.\n",
      "**************************************************\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 10 articles on page 1 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 10 articles on page 2 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 10 articles on page 3 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 10 articles on page 4 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 10 articles on page 5 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 10 articles on page 6 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 10 articles on page 7 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 8 of category 'world'...\n",
      "\t--> Found 10 articles on page 8 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 9 of category 'world'...\n",
      "\t--> Found 10 articles on page 9 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 9 of 'world'.\n",
      "**************************************************\n",
      "Scraping page 10 of category 'world'...\n",
      "\t--> Found 10 articles on page 10 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 10 of 'world'.\n",
      "**************************************************\n",
      "Scraping completed. Sample data:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the scraper\n",
    "    scraper = ExpressScraper()\n",
    "\n",
    "    # Run the scraper and fetch articles\n",
    "    try:\n",
    "        express_df = scraper.get_express_articles(max_pages=10) \n",
    "        print(\"Scraping completed. Sample data:\")\n",
    "\n",
    "        # Display the first few rows of the scraped data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during scraping:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcff1e9b-0717-4504-8232-cfd090e9b0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>نیٹ فلکس ڈاؤن، سوشل میڈیا پر شکایتوں کا طوفان ...</td>\n",
       "      <td>https://www.express.pk/story/2733439/netflix-d...</td>\n",
       "      <td>معروف امریکی اسٹریمنگ پلیٹ فارم نیٹ فلکس باکسن...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>پاکستانی ویب سیریز ’فروٹ چاٹ‘ کی شوٹنگ کا آغا...</td>\n",
       "      <td>https://www.express.pk/story/2733408/the-shoot...</td>\n",
       "      <td>عالمی ایوارڈ یافتہ پاکستان شوبز کے نوجوان ہدای...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ریاض میں رنگارنگ فیشن شو، جینیفر لوپیز سمیت دی...</td>\n",
       "      <td>https://www.express.pk/story/2733380/a-colorfu...</td>\n",
       "      <td>سعودی عرب کےدارالحکومتریاض میں منعقد ہونے والے...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>دیشا پٹانی کے والد کے ساتھ لاکھوں روپے کا فراڈ...</td>\n",
       "      <td>https://www.express.pk/story/2733374/disha-pat...</td>\n",
       "      <td>بالی ووڈ  کی خوبرو اداکارہ دیشا پٹانی کے والد،...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ڈان 3 میں ولن کون ہوگا؟ نام سامنے آگیا</td>\n",
       "      <td>https://www.express.pk/story/2733372/who-will-...</td>\n",
       "      <td>بالی ووڈ کی مشہور فلم ’ڈان‘ فرنچائز کی تیسری ف...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>سوئٹزرلینڈ؛ نقاب پر پابندی اور خلاف ورزی پر سز...</td>\n",
       "      <td>https://www.express.pk/story/2732032/switzerla...</td>\n",
       "      <td>سوئٹزرلینڈ میں 'برقع پر پابندی' کے بل کا اطلاق...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>ٹرمپ اسرائیل کی حماس اور لبنان کیساتھ جنگوں کا...</td>\n",
       "      <td>https://www.express.pk/story/2732020/donald-tr...</td>\n",
       "      <td>امریکی صدارتی الیکشن کی فاتح جماعت ریپبلکن کی ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>اسرائیل کی لبنان کے تاریخی شہر پر بمباری؛ 40 ا...</td>\n",
       "      <td>https://www.express.pk/story/2732009/israel-at...</td>\n",
       "      <td>اسرائیل نے لبنان کے مشرقی شہر بعلبک میں تاریخی...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>امریکا کی نئی’سیکنڈ لیڈی‘ بھارتی نژاد خاتون ہیں</td>\n",
       "      <td>https://www.express.pk/story/2731989/uselectio...</td>\n",
       "      <td>ڈونلڈ ٹرمپ نے صدارتی الیکشن میں کامیابی کے بعد...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>حسیناؤں کے جھرمٹ میں رہنے والے ٹرمپ کے معاشقے...</td>\n",
       "      <td>https://www.express.pk/story/2731988/donald-tr...</td>\n",
       "      <td>اپنی رنگین مزاجی، مضحکہ خیز بیانات اور جارحانہ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0      0  نیٹ فلکس ڈاؤن، سوشل میڈیا پر شکایتوں کا طوفان ...   \n",
       "1      1  پاکستانی ویب سیریز ’فروٹ چاٹ‘ کی شوٹنگ کا آغا...   \n",
       "2      2  ریاض میں رنگارنگ فیشن شو، جینیفر لوپیز سمیت دی...   \n",
       "3      3  دیشا پٹانی کے والد کے ساتھ لاکھوں روپے کا فراڈ...   \n",
       "4      4            ڈان 3 میں ولن کون ہوگا؟ نام سامنے آگیا   \n",
       "..   ...                                                ...   \n",
       "495  495  سوئٹزرلینڈ؛ نقاب پر پابندی اور خلاف ورزی پر سز...   \n",
       "496  496  ٹرمپ اسرائیل کی حماس اور لبنان کیساتھ جنگوں کا...   \n",
       "497  497  اسرائیل کی لبنان کے تاریخی شہر پر بمباری؛ 40 ا...   \n",
       "498  498    امریکا کی نئی’سیکنڈ لیڈی‘ بھارتی نژاد خاتون ہیں   \n",
       "499  499  حسیناؤں کے جھرمٹ میں رہنے والے ٹرمپ کے معاشقے...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    https://www.express.pk/story/2733439/netflix-d...   \n",
       "1    https://www.express.pk/story/2733408/the-shoot...   \n",
       "2    https://www.express.pk/story/2733380/a-colorfu...   \n",
       "3    https://www.express.pk/story/2733374/disha-pat...   \n",
       "4    https://www.express.pk/story/2733372/who-will-...   \n",
       "..                                                 ...   \n",
       "495  https://www.express.pk/story/2732032/switzerla...   \n",
       "496  https://www.express.pk/story/2732020/donald-tr...   \n",
       "497  https://www.express.pk/story/2732009/israel-at...   \n",
       "498  https://www.express.pk/story/2731989/uselectio...   \n",
       "499  https://www.express.pk/story/2731988/donald-tr...   \n",
       "\n",
       "                                               content     gold_label  \n",
       "0    معروف امریکی اسٹریمنگ پلیٹ فارم نیٹ فلکس باکسن...  entertainment  \n",
       "1    عالمی ایوارڈ یافتہ پاکستان شوبز کے نوجوان ہدای...  entertainment  \n",
       "2    سعودی عرب کےدارالحکومتریاض میں منعقد ہونے والے...  entertainment  \n",
       "3    بالی ووڈ  کی خوبرو اداکارہ دیشا پٹانی کے والد،...  entertainment  \n",
       "4    بالی ووڈ کی مشہور فلم ’ڈان‘ فرنچائز کی تیسری ف...  entertainment  \n",
       "..                                                 ...            ...  \n",
       "495  سوئٹزرلینڈ میں 'برقع پر پابندی' کے بل کا اطلاق...          world  \n",
       "496  امریکی صدارتی الیکشن کی فاتح جماعت ریپبلکن کی ...          world  \n",
       "497  اسرائیل نے لبنان کے مشرقی شہر بعلبک میں تاریخی...          world  \n",
       "498  ڈونلڈ ٹرمپ نے صدارتی الیکشن میں کامیابی کے بعد...          world  \n",
       "499  اپنی رنگین مزاجی، مضحکہ خیز بیانات اور جارحانہ...          world  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "express_df.head()\n",
    "express_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60c762e-2376-4011-b489-1a045d17adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'fun-o-sakafat'...\n",
      "Scraping page 2 of category 'fun-o-sakafat'...\n",
      "Scraping page 3 of category 'fun-o-sakafat'...\n",
      "Scraping page 4 of category 'fun-o-sakafat'...\n",
      "Scraping page 5 of category 'fun-o-sakafat'...\n",
      "Scraping page 1 of category 'کاروباری-خبریں'...\n",
      "Scraping page 2 of category 'کاروباری-خبریں'...\n",
      "Scraping page 3 of category 'کاروباری-خبریں'...\n",
      "Scraping page 4 of category 'کاروباری-خبریں'...\n",
      "Scraping page 5 of category 'کاروباری-خبریں'...\n",
      "Scraping page 1 of category 'sports-2'...\n",
      "Scraping page 2 of category 'sports-2'...\n",
      "Scraping page 3 of category 'sports-2'...\n",
      "Scraping page 4 of category 'sports-2'...\n",
      "Scraping page 5 of category 'sports-2'...\n",
      "Scraping page 1 of category 'سائنس-اور-ٹیکنالوجی'...\n",
      "Scraping page 2 of category 'سائنس-اور-ٹیکنالوجی'...\n",
      "Scraping page 3 of category 'سائنس-اور-ٹیکنالوجی'...\n",
      "Scraping page 4 of category 'سائنس-اور-ٹیکنالوجی'...\n",
      "Scraping page 5 of category 'سائنس-اور-ٹیکنالوجی'...\n",
      "Scraping page 1 of category 'international-2'...\n",
      "Scraping page 2 of category 'international-2'...\n",
      "Scraping page 3 of category 'international-2'...\n",
      "Scraping page 4 of category 'international-2'...\n",
      "Scraping page 5 of category 'international-2'...\n",
      "   id                                              title  \\\n",
      "0   1  ہم لکھنے والے ایک بے ایمان معاشرہ میں سانس لے ...   \n",
      "1   2                          ’’کیا اُدھر باغ بھی ہے؟‘‘   \n",
      "2   3               کچھ زخمی کہہ رہے تھے کہ وہ زندہ ہیں…   \n",
      "3   4               آگرے والا عبدالکریم اور ملکہ وکٹوریہ   \n",
      "4   5                                \"چچی سیدانی تھیں….”   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://urdu.arynews.tv/intezar-hussain-urdu-w...   \n",
      "1   https://urdu.arynews.tv/lakhnau-nawab-mahalsara/   \n",
      "2  https://urdu.arynews.tv/politics-and-politicia...   \n",
      "3  https://urdu.arynews.tv/munshi-abdul-karim-and...   \n",
      "4        https://urdu.arynews.tv/aslam-parvez-khaka/   \n",
      "\n",
      "                                             content     gold_label  \n",
      "0  میں سوچتا ہوں کہ ہم غالب سے کتنے مختلف زمانے م...  entertainment  \n",
      "1  محمد حسین آزاد نے میر کے بارے میں ایک واقعہ ل...  entertainment  \n",
      "2  وطنِ عزیز کی سیاست کا دار و مدار اب اس پر رہ گ...  entertainment  \n",
      "3  بہت پیارے شوہر البرٹ اور بے حد عزیز ملازم جان ...  entertainment  \n",
      "4  خاکہ نگاری اردو میں‌ ایک مقبول صنف رہی ہے۔ زیا...  entertainment  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "class AryNewsScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://urdu.arynews.tv'\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        self.categories = [\n",
    "            ('fun-o-sakafat', 'entertainment'),\n",
    "            ('کاروباری-خبریں', 'business'),\n",
    "            ('sports-2', 'sports'),\n",
    "            ('سائنس-اور-ٹیکنالوجی', 'science-technology'),\n",
    "            ('international-2', 'world')\n",
    "        ]\n",
    "        self.id = 1\n",
    "    \n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Fetches a single page and handles errors.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_article(self, article_url):\n",
    "        \"\"\"Fetch and parse the article content.\"\"\"\n",
    "        article_response = self.fetch_page(article_url)\n",
    "        if not article_response:\n",
    "            return None\n",
    "\n",
    "        content_soup = BeautifulSoup(article_response, \"html.parser\")\n",
    "        paragraphs = content_soup.find('div', class_='vc_column-inner').find_all('p')\n",
    "        return \" \".join([p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '') for p in paragraphs if p.get_text(strip=True)])\n",
    "\n",
    "    def scrape_category(self, category, category_label, max_pages=7):\n",
    "        \"\"\"Scrape articles from a given category.\"\"\"\n",
    "        ary_data = []\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            print(f\"Scraping page {page} of category '{category}'...\")\n",
    "            url = f\"{self.base_url}/category/{category}/page/{page}\"\n",
    "            page_html = self.fetch_page(url)\n",
    "            if not page_html:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "            cards = soup.find_all('div', class_='tdb_module_loop td_module_wrap td-animation-stack td-cpt-post')\n",
    "\n",
    "            for card in cards:\n",
    "                try:\n",
    "                    headline = card.find('a')['title']\n",
    "                    link = card.find('a')['href']\n",
    "                    content = self.parse_article(link)\n",
    "\n",
    "                    if content:  # Only append if content is available\n",
    "                        ary_data.append({\n",
    "                            \"id\": self.id,\n",
    "                            \"title\": headline,\n",
    "                            \"link\": link,\n",
    "                            \"content\": content,\n",
    "                            \"gold_label\": category_label\n",
    "                        })\n",
    "                        self.id += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article on page {page}: {e}\")\n",
    "\n",
    "            sleep(randint(1, 3))  # Random sleep to avoid rate limiting\n",
    "\n",
    "        return ary_data\n",
    "\n",
    "    def scrape_all(self, max_pages=7):\n",
    "        \"\"\"Scrape all categories and compile the data.\"\"\"\n",
    "        all_data = []\n",
    "        for category, label in self.categories:\n",
    "            data = self.scrape_category(category, label, max_pages)\n",
    "            all_data.extend(data)\n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "scraper = AryNewsScraper()\n",
    "articles_df = scraper.scrape_all(max_pages=5)\n",
    "print(articles_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e84497-fbd4-4f07-ba19-b35c5885c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t--> Found 18 articles on 'Entertainment'.\n",
      "\t--> Successfully scraped 18 articles of 'Entertainment'.\n",
      "\t--> Found 18 articles on 'Business'.\n",
      "\t--> Successfully scraped 18 articles of 'Business'.\n",
      "\t--> Found 18 articles on 'Sports'.\n",
      "\t--> Successfully scraped 18 articles of 'Sports'.\n",
      "\t--> Found 18 articles on 'Technology'.\n",
      "\t--> Successfully scraped 18 articles of 'Technology'.\n",
      "\t--> Found 18 articles on 'World'.\n",
      "\t--> Successfully scraped 18 articles of 'World'.\n",
      "Scraping completed. Sample data:\n",
      "   id                                              title  \\\n",
      "0   0  عزیز میاں قوال کو دنیا سے رخصت ہوئے 24 برس بیت...   \n",
      "1   1  ماضی کے مقبول گلوکار اور معروف نعت خواں جنید ج...   \n",
      "2   2  یونیسیف کی سفیر برائے اطفال صبا قمر کی جرمن قو...   \n",
      "3   3  بالی وڈ اداکار آدتیہ پنچولی کا بعد ازموت جسم ع...   \n",
      "4   4  مایا علی کے مایوں لک میں حسن کے جلوے، مداح دل ...   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://urdu.dunyanews.tv/index.php/ur/Enterta...   \n",
      "1  https://urdu.dunyanews.tv/index.php/ur/Enterta...   \n",
      "2  https://urdu.dunyanews.tv/index.php/ur/Enterta...   \n",
      "3  https://urdu.dunyanews.tv/index.php/ur/Enterta...   \n",
      "4  https://urdu.dunyanews.tv/index.php/ur/Enterta...   \n",
      "\n",
      "                                             content     gold_label  \n",
      "0  لاہور: (دنیا نیوز) رومانوی اور صوفیانہ کلام کو...  Entertainment  \n",
      "1  لاہور: (دنیا نیوز) ماضی کے مقبول گلوکار اور مع...  Entertainment  \n",
      "2  کراچی: (دنیا نیوز) یونیسیف کی جانب سے پاکستان ...  Entertainment  \n",
      "3  نئی دہلی :(دنیا نیوز) بالی وڈ کے معروف اداکار ...  Entertainment  \n",
      "4  لاہور: (ویب ڈیسک) خوبرو پاکستانی اداکارہ مایا ...  Entertainment  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class NewsScraper2:\n",
    "    def __init__(self):\n",
    "        self.id = 0  # Initialize the article id\n",
    "        \n",
    "    def get_dunya_articles(self, max_pages=7):\n",
    "        dunya_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        \n",
    "        base_url = 'https://urdu.dunyanews.tv'\n",
    "        categories = ['Entertainment', 'Business', 'Sports', 'Technology', 'World']\n",
    "        \n",
    "        # Start scraping categories with a while loop\n",
    "        cat_index = 0\n",
    "        while cat_index < len(categories):\n",
    "            category = categories[cat_index]\n",
    "            url = f\"{base_url}/index.php/ur/{category}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "            cards = soup.find('div', class_='newsBox categories').find_all('div', class_='cNewsBox') \n",
    "            print(f\"\\t--> Found {len(cards)} articles on '{category}'.\")\n",
    "            success_count = 0\n",
    "            article_index = 0\n",
    "            \n",
    "            while article_index < len(cards):\n",
    "                try:\n",
    "                    card = cards[article_index]\n",
    "                    div = card.find('div', class_='col-md-8')\n",
    "                    \n",
    "                    headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "                    link = div.find('a')['href']\n",
    "                    targetlink = base_url + link\n",
    "                    \n",
    "                    article_response = requests.get(targetlink)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                    \n",
    "                    paras = content_soup.find('div', class_='main-news col-md-12').find_all('p')\n",
    "                    lines = ''\n",
    "                    para_index = 0\n",
    "                    while para_index < len(paras):\n",
    "                        p = paras[para_index]\n",
    "                        lines += p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        para_index += 1\n",
    "                    combined_text = \"\".join(lines)\n",
    "                    \n",
    "                    dunya_df['id'].append(self.id)\n",
    "                    dunya_df['title'].append(headline)\n",
    "                    dunya_df['link'].append(targetlink)\n",
    "                    dunya_df['gold_label'].append(category.replace('Technology','Science-Technology'))\n",
    "                    dunya_df['content'].append(combined_text)\n",
    "                   \n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape an article on '{category}': {e}\")\n",
    "                    \n",
    "                article_index += 1\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles of '{category}'.\")\n",
    "            cat_index += 1\n",
    "\n",
    "        return pd.DataFrame(dunya_df)\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the scraper\n",
    "    scraper = NewsScraper2()\n",
    "\n",
    "    # Run the scraper and fetch articles\n",
    "    try:\n",
    "        dunya_df = scraper.get_dunya_articles(max_pages=10)\n",
    "        print(\"Scraping completed. Sample data:\")\n",
    "        \n",
    "        # Display the first few rows of the scraped data\n",
    "        print(dunya_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during scraping:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7117dd0f-a4a2-445e-bae6-a63bf33cba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def get_jang_articles(self):\n",
    "        jang_data = {\n",
    "            \"article_id\": [],\n",
    "            \"headline\": [],\n",
    "            \"url\": [],\n",
    "            \"body_content\": [],\n",
    "            \"category_label\": []\n",
    "        }\n",
    "\n",
    "        base_url = \"https://jang.com.pk/category/latest-news\"\n",
    "        jang_sections = {\n",
    "            'showbiz': 'entertainment',\n",
    "            'markets': 'business',\n",
    "            'fitness': 'sports',\n",
    "            'tech': 'technology',\n",
    "            'international': 'world'\n",
    "        }\n",
    "\n",
    "        section_index = 0\n",
    "        section_keys = list(jang_sections.keys())\n",
    "        \n",
    "        while section_index < len(section_keys):\n",
    "            section_key = section_keys[section_index]\n",
    "            category = jang_sections[section_key]\n",
    "            url = f\"{base_url}/{section_key}\"\n",
    "            print(f\"Scraping '{category}' section...\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.find(\"ul\", class_=\"scrollPaginationNew__\").find_all(\"li\")\n",
    "                article_index = 0\n",
    "                \n",
    "                while article_index < len(articles):\n",
    "                    article = articles[article_index]\n",
    "                    try:\n",
    "                        link_tag = article.find(\"a\")\n",
    "                        article_url = link_tag[\"href\"] if link_tag else None\n",
    "                        headline = link_tag.get(\"title\", \"\").strip() if link_tag else None\n",
    "                        article_id = article.get(\"data-id\", None)\n",
    "\n",
    "                        if article_url:\n",
    "                            article_response = requests.get(article_url)\n",
    "                            article_response.raise_for_status()\n",
    "                            content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                            paragraphs = content_soup.find_all(\"p\")\n",
    "                            body_content = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                        else:\n",
    "                            body_content = \"\"\n",
    "\n",
    "                        jang_data[\"article_id\"].append(article_id)\n",
    "                        jang_data[\"headline\"].append(headline)\n",
    "                        jang_data[\"url\"].append(article_url)\n",
    "                        jang_data[\"body_content\"].append(body_content)\n",
    "                        jang_data[\"category_label\"].append(category)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article: {e}\")\n",
    "\n",
    "                    article_index += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch '{category}', error: {e}\")\n",
    "\n",
    "            section_index += 1\n",
    "\n",
    "        return pd.DataFrame(jang_data)\n",
    "\n",
    "    def get_geo_articles(self, max_pages=7):\n",
    "        geo_data = {\n",
    "            \"article_id\": [],\n",
    "            \"headline\": [],\n",
    "            \"url\": [],\n",
    "            \"body_content\": [],\n",
    "            \"category_label\": []\n",
    "        }\n",
    "        base_url = 'https://urdu.geo.tv'\n",
    "        geo_sections = ['entertainment', 'business', 'sports', 'science-technology', 'world']\n",
    "\n",
    "        section_index = 0\n",
    "        while section_index < len(geo_sections):\n",
    "            section = geo_sections[section_index]\n",
    "            url = f\"{base_url}/category/{section}\"\n",
    "            print(f\"Scraping '{section}' section...\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.find('div', class_='video-list laodMoreCatNews').find_all(\n",
    "                    'div', class_='col-xs-6 col-sm-6 col-lg-6 col-md-6 singleBlock'\n",
    "                )\n",
    "                article_index = 0\n",
    "                \n",
    "                while article_index < len(articles):\n",
    "                    article = articles[article_index]\n",
    "                    try:\n",
    "                        headline = article.find('h2').get_text(strip=True)\n",
    "                        article_url = article.find('a')['href']\n",
    "                        article_id = article.get(\"data-id\", None)\n",
    "                        article_response = requests.get(article_url)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        paragraphs = content_soup.find('div', class_='content-area').find_all('p')\n",
    "                        body_content = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "                        geo_data['article_id'].append(article_id)\n",
    "                        geo_data['headline'].append(headline)\n",
    "                        geo_data['url'].append(article_url)\n",
    "                        geo_data['body_content'].append(body_content)\n",
    "                        geo_data['category_label'].append(section)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape article in '{section}': {e}\")\n",
    "\n",
    "                    article_index += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fetch '{section}', error: {e}\")\n",
    "\n",
    "            section_index += 1\n",
    "\n",
    "        return pd.DataFrame(geo_data)\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the NewsScraper instance\n",
    "    scraper = NewsScraper()\n",
    "    \n",
    "    # Test scraping articles from Jang\n",
    "    print(\"Testing Jang Articles Scraping...\")\n",
    "    jang_articles_df = scraper.get_jang_articles()\n",
    "    print(f\"Scraped {len(jang_articles_df)} articles from Jang.\")\n",
    "    print(jang_articles_df.head())\n",
    "\n",
    "    # Test scraping articles from Geo\n",
    "    print(\"\\nTesting Geo Articles Scraping...\")\n",
    "    geo_articles_df = scraper.get_geo_articles()\n",
    "    print(f\"Scraped {len(geo_articles_df)} articles from Geo.\")\n",
    "    print(geo_articles_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937bc7f8-523b-4b40-bf02-534f9d5e094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NewsScraper instance\n",
    "scraper = NewsScraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49d8fd1-643c-4189-8cef-497e713b0920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping from Jang:\n",
      "\n",
      "Scraping 'entertainment' section...\n",
      "Scraping 'business' section...\n",
      "Scraping 'sports' section...\n",
      "Scraping 'technology' section...\n",
      "Scraping 'world' section...\n",
      "     id                                              title  \\\n",
      "0  None  آسکر ایوارڈ یافتہ بھارتی موسیقار اے آر رحمٰن ...   \n",
      "1  None  کمی کے باوجود اینٹی اسموگ مہم جاری رکھنے کا اعلان   \n",
      "2  None  بچوں کو مشاورت میں شریک کرنے کی تہذیب کو فروغ ...   \n",
      "3  None  گزشتہ حکومت پوچھنے پر بھی توشہ خانے کی تفصیلات...   \n",
      "4  None  ڈونلڈ ٹرمپ نے ڈبلیو ڈبلیو ای کی سابق سی ای او ...   \n",
      "\n",
      "                               link  \\\n",
      "0  https://jang.com.pk/news/1412845   \n",
      "1  https://jang.com.pk/news/1412843   \n",
      "2  https://jang.com.pk/news/1412842   \n",
      "3  https://jang.com.pk/news/1412841   \n",
      "4  https://jang.com.pk/news/1412840   \n",
      "\n",
      "                                             content     gold_label  \n",
      "0   اسکر ایوارڈ یافتہ بھارتی معروف موسیقار و گلوک...  entertainment  \n",
      "1   پنجاب کی سینئر وزیر مریم اورنگزیب نے کہا ہے ک...  entertainment  \n",
      "2   وزیرِ اعلیٰ سندھ مراد علی شاہ نے بچوں کے عالم...  entertainment  \n",
      "3   بانئ پی ٹی آئی کی توشہ خانہ کیس ٹو میں ضمانت ...  entertainment  \n",
      "4   ڈونلڈ ٹرمپ نے ورلڈ ریسلنگ انٹرٹینمنٹ (ڈبلیو ڈ...  entertainment  \n"
     ]
    }
   ],
   "source": [
    "# Test scraping from Jang\n",
    "print(\"\\nScraping from Jang:\\n\")\n",
    "jang_df = scraper.get_jang_articles()\n",
    "print(jang_df.head())  # Displaying the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "278153ac-c37a-4917-9aa8-b820314839ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>آسکر ایوارڈ یافتہ بھارتی موسیقار اے آر رحمٰن ...</td>\n",
       "      <td>https://jang.com.pk/news/1412845</td>\n",
       "      <td>اسکر ایوارڈ یافتہ بھارتی معروف موسیقار و گلوک...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>کمی کے باوجود اینٹی اسموگ مہم جاری رکھنے کا اعلان</td>\n",
       "      <td>https://jang.com.pk/news/1412843</td>\n",
       "      <td>پنجاب کی سینئر وزیر مریم اورنگزیب نے کہا ہے ک...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>بچوں کو مشاورت میں شریک کرنے کی تہذیب کو فروغ ...</td>\n",
       "      <td>https://jang.com.pk/news/1412842</td>\n",
       "      <td>وزیرِ اعلیٰ سندھ مراد علی شاہ نے بچوں کے عالم...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>گزشتہ حکومت پوچھنے پر بھی توشہ خانے کی تفصیلات...</td>\n",
       "      <td>https://jang.com.pk/news/1412841</td>\n",
       "      <td>بانئ پی ٹی آئی کی توشہ خانہ کیس ٹو میں ضمانت ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>ڈونلڈ ٹرمپ نے ڈبلیو ڈبلیو ای کی سابق سی ای او ...</td>\n",
       "      <td>https://jang.com.pk/news/1412840</td>\n",
       "      <td>ڈونلڈ ٹرمپ نے ورلڈ ریسلنگ انٹرٹینمنٹ (ڈبلیو ڈ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>None</td>\n",
       "      <td>اے ڈی بی قرض پروگرام کی رقم موسمیاتی تبدیلی سے...</td>\n",
       "      <td>https://jang.com.pk/news/1412537</td>\n",
       "      <td>اقتصادی امور کے ترجمان  کا کہنا ہے کہ پاکستان...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>None</td>\n",
       "      <td>گورنر پنجاب سلیم حیدر خان کا طلباء یونینز کی ب...</td>\n",
       "      <td>https://jang.com.pk/news/1412536</td>\n",
       "      <td>گورنر پنجاب سلیم حیدر خان کا کہنا ہے کہ طلباء...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>None</td>\n",
       "      <td>سرگودھا: پشاور جانیوالی مسافر بس میں آگ لگ گئی</td>\n",
       "      <td>https://jang.com.pk/news/1412535</td>\n",
       "      <td>سرگودھا کے علاقے چناب پل کے قریب مسافر بس میں...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>None</td>\n",
       "      <td>بھارت: انتہائی کم قیمت پر بیچا گیا ادرک لہسن ک...</td>\n",
       "      <td>https://jang.com.pk/news/1412534</td>\n",
       "      <td>بھارت کی ریاست تلنگانہ میں انتہائی کم قیمت پر...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>None</td>\n",
       "      <td>کراچی: بجلی کے بلوں میں اضافی سرچارج کیخلاف در...</td>\n",
       "      <td>https://jang.com.pk/news/1412533</td>\n",
       "      <td>کراچی میں بجلی کے بلوں میں اضافی سرچارج کے خل...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0    None  آسکر ایوارڈ یافتہ بھارتی موسیقار اے آر رحمٰن ...   \n",
       "1    None  کمی کے باوجود اینٹی اسموگ مہم جاری رکھنے کا اعلان   \n",
       "2    None  بچوں کو مشاورت میں شریک کرنے کی تہذیب کو فروغ ...   \n",
       "3    None  گزشتہ حکومت پوچھنے پر بھی توشہ خانے کی تفصیلات...   \n",
       "4    None  ڈونلڈ ٹرمپ نے ڈبلیو ڈبلیو ای کی سابق سی ای او ...   \n",
       "..    ...                                                ...   \n",
       "505  None  اے ڈی بی قرض پروگرام کی رقم موسمیاتی تبدیلی سے...   \n",
       "506  None  گورنر پنجاب سلیم حیدر خان کا طلباء یونینز کی ب...   \n",
       "507  None    سرگودھا: پشاور جانیوالی مسافر بس میں آگ لگ گئی   \n",
       "508  None  بھارت: انتہائی کم قیمت پر بیچا گیا ادرک لہسن ک...   \n",
       "509  None  کراچی: بجلی کے بلوں میں اضافی سرچارج کیخلاف در...   \n",
       "\n",
       "                                 link  \\\n",
       "0    https://jang.com.pk/news/1412845   \n",
       "1    https://jang.com.pk/news/1412843   \n",
       "2    https://jang.com.pk/news/1412842   \n",
       "3    https://jang.com.pk/news/1412841   \n",
       "4    https://jang.com.pk/news/1412840   \n",
       "..                                ...   \n",
       "505  https://jang.com.pk/news/1412537   \n",
       "506  https://jang.com.pk/news/1412536   \n",
       "507  https://jang.com.pk/news/1412535   \n",
       "508  https://jang.com.pk/news/1412534   \n",
       "509  https://jang.com.pk/news/1412533   \n",
       "\n",
       "                                               content     gold_label  \n",
       "0     اسکر ایوارڈ یافتہ بھارتی معروف موسیقار و گلوک...  entertainment  \n",
       "1     پنجاب کی سینئر وزیر مریم اورنگزیب نے کہا ہے ک...  entertainment  \n",
       "2     وزیرِ اعلیٰ سندھ مراد علی شاہ نے بچوں کے عالم...  entertainment  \n",
       "3     بانئ پی ٹی آئی کی توشہ خانہ کیس ٹو میں ضمانت ...  entertainment  \n",
       "4     ڈونلڈ ٹرمپ نے ورلڈ ریسلنگ انٹرٹینمنٹ (ڈبلیو ڈ...  entertainment  \n",
       "..                                                 ...            ...  \n",
       "505   اقتصادی امور کے ترجمان  کا کہنا ہے کہ پاکستان...          world  \n",
       "506   گورنر پنجاب سلیم حیدر خان کا کہنا ہے کہ طلباء...          world  \n",
       "507   سرگودھا کے علاقے چناب پل کے قریب مسافر بس میں...          world  \n",
       "508   بھارت کی ریاست تلنگانہ میں انتہائی کم قیمت پر...          world  \n",
       "509   کراچی میں بجلی کے بلوں میں اضافی سرچارج کے خل...          world  \n",
       "\n",
       "[510 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jang_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e396e428-fa2c-4877-b40d-38ede8cc8573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping from GEO:\n",
      "\n",
      "Scraping 'entertainment' section...\n",
      "Scraping 'business' section...\n",
      "Scraping 'sports' section...\n",
      "Scraping 'science-technology' section...\n",
      "Scraping 'world' section...\n",
      "    id                                              title  \\\n",
      "0  510  عدنان صدیقی کی برطانوی بادشاہ سے ملاقات، شاہ چ...   \n",
      "1  511  سعودی دارالحکومت ریاض میں فیشن شو، معروف گلوکا...   \n",
      "2  512  پاکستانی مداح نے کنسرٹ میں میکا سنگھ کو کروڑوں...   \n",
      "3  513  سدھو نے  کپل شرما شو چھوڑنے کی اصل وجہ 5 سال ب...   \n",
      "4  514  امیتابھ بچن بیٹے ابھیشیک کو اپنے شو میں بلاکر ...   \n",
      "\n",
      "                                 link  \\\n",
      "0  https://urdu.geo.tv/latest/387006-   \n",
      "1  https://urdu.geo.tv/latest/386962-   \n",
      "2  https://urdu.geo.tv/latest/386939-   \n",
      "3  https://urdu.geo.tv/latest/386888-   \n",
      "4  https://urdu.geo.tv/latest/386887-   \n",
      "\n",
      "                                             content     gold_label  \n",
      "0  پاکستانی شوبز  انڈسٹری کے سینئر اداکار عدنان ص...  entertainment  \n",
      "1    سعودی عرب کے دارالحکومت ریاض میں فیشن شو کا ...  entertainment  \n",
      "2  بالی وڈ کے معروف گلوکار میکا سنگھ کو  کنسرٹ می...  entertainment  \n",
      "3  بھارتی ریاست پنجاب کے وزیر و سابق کرکٹر نووجوت...  entertainment  \n",
      "4  بالی وڈ کے بگ بی امیتابھ بچن ’کون بنے گا کروڑ ...  entertainment  \n"
     ]
    }
   ],
   "source": [
    "# Test scraping from GEO\n",
    "print(\"\\nScraping from GEO:\\n\")\n",
    "geo_df = scraper.get_geo_articles(max_pages=2)\n",
    "print(geo_df.head())  # Displaying the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fa2e6f7-f340-4ce6-8bfe-ac55e920422e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>عدنان صدیقی کی برطانوی بادشاہ سے ملاقات، شاہ چ...</td>\n",
       "      <td>https://urdu.geo.tv/latest/387006-</td>\n",
       "      <td>پاکستانی شوبز  انڈسٹری کے سینئر اداکار عدنان ص...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>511</td>\n",
       "      <td>سعودی دارالحکومت ریاض میں فیشن شو، معروف گلوکا...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386962-</td>\n",
       "      <td>سعودی عرب کے دارالحکومت ریاض میں فیشن شو کا ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512</td>\n",
       "      <td>پاکستانی مداح نے کنسرٹ میں میکا سنگھ کو کروڑوں...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386939-</td>\n",
       "      <td>بالی وڈ کے معروف گلوکار میکا سنگھ کو  کنسرٹ می...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>513</td>\n",
       "      <td>سدھو نے  کپل شرما شو چھوڑنے کی اصل وجہ 5 سال ب...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386888-</td>\n",
       "      <td>بھارتی ریاست پنجاب کے وزیر و سابق کرکٹر نووجوت...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>514</td>\n",
       "      <td>امیتابھ بچن بیٹے ابھیشیک کو اپنے شو میں بلاکر ...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386887-</td>\n",
       "      <td>بالی وڈ کے بگ بی امیتابھ بچن ’کون بنے گا کروڑ ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>805</td>\n",
       "      <td>حطیم  میں نوافل کی ادائیگی کے اوقات  جاری</td>\n",
       "      <td>https://urdu.geo.tv/latest/386429-</td>\n",
       "      <td>مکہ مکرمہ:  خانہ کعبہ کی حدود حطیم  میں نوافل ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>806</td>\n",
       "      <td>غزہ میں مزاحمت کاروں کے حملے میں 4 اسرائیلی فو...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386427-</td>\n",
       "      <td>تل ابیب: غزہ میں فلسطینی مزاحمت کاروں کے حمل...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>807</td>\n",
       "      <td>ٹرمپ کے ’47 ویں‘ امریکی صدر بننے کی راہ میں کم...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386411-</td>\n",
       "      <td>امریکا میں 5 نومبر کو ہونے والے صدارتی انتخابا...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>808</td>\n",
       "      <td>روس کا آبادی میں اضافے کیلئے اقدامات پر غور، ...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386410-</td>\n",
       "      <td>روس نے ملک کی آبادی میں اضافے کیلئے خصوصی وزا...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>809</td>\n",
       "      <td>بین الاقوامی برادری ایران کی سالمیت اور خود مخ...</td>\n",
       "      <td>https://urdu.geo.tv/latest/386396-</td>\n",
       "      <td>ریاض: سعودی ولی عہد شہزادہ محمد بن سلمان نے کہ...</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0    510  عدنان صدیقی کی برطانوی بادشاہ سے ملاقات، شاہ چ...   \n",
       "1    511  سعودی دارالحکومت ریاض میں فیشن شو، معروف گلوکا...   \n",
       "2    512  پاکستانی مداح نے کنسرٹ میں میکا سنگھ کو کروڑوں...   \n",
       "3    513  سدھو نے  کپل شرما شو چھوڑنے کی اصل وجہ 5 سال ب...   \n",
       "4    514  امیتابھ بچن بیٹے ابھیشیک کو اپنے شو میں بلاکر ...   \n",
       "..   ...                                                ...   \n",
       "295  805          حطیم  میں نوافل کی ادائیگی کے اوقات  جاری   \n",
       "296  806  غزہ میں مزاحمت کاروں کے حملے میں 4 اسرائیلی فو...   \n",
       "297  807  ٹرمپ کے ’47 ویں‘ امریکی صدر بننے کی راہ میں کم...   \n",
       "298  808  روس کا آبادی میں اضافے کیلئے اقدامات پر غور، ...   \n",
       "299  809  بین الاقوامی برادری ایران کی سالمیت اور خود مخ...   \n",
       "\n",
       "                                   link  \\\n",
       "0    https://urdu.geo.tv/latest/387006-   \n",
       "1    https://urdu.geo.tv/latest/386962-   \n",
       "2    https://urdu.geo.tv/latest/386939-   \n",
       "3    https://urdu.geo.tv/latest/386888-   \n",
       "4    https://urdu.geo.tv/latest/386887-   \n",
       "..                                  ...   \n",
       "295  https://urdu.geo.tv/latest/386429-   \n",
       "296  https://urdu.geo.tv/latest/386427-   \n",
       "297  https://urdu.geo.tv/latest/386411-   \n",
       "298  https://urdu.geo.tv/latest/386410-   \n",
       "299  https://urdu.geo.tv/latest/386396-   \n",
       "\n",
       "                                               content     gold_label  \n",
       "0    پاکستانی شوبز  انڈسٹری کے سینئر اداکار عدنان ص...  entertainment  \n",
       "1      سعودی عرب کے دارالحکومت ریاض میں فیشن شو کا ...  entertainment  \n",
       "2    بالی وڈ کے معروف گلوکار میکا سنگھ کو  کنسرٹ می...  entertainment  \n",
       "3    بھارتی ریاست پنجاب کے وزیر و سابق کرکٹر نووجوت...  entertainment  \n",
       "4    بالی وڈ کے بگ بی امیتابھ بچن ’کون بنے گا کروڑ ...  entertainment  \n",
       "..                                                 ...            ...  \n",
       "295  مکہ مکرمہ:  خانہ کعبہ کی حدود حطیم  میں نوافل ...          world  \n",
       "296    تل ابیب: غزہ میں فلسطینی مزاحمت کاروں کے حمل...          world  \n",
       "297  امریکا میں 5 نومبر کو ہونے والے صدارتی انتخابا...          world  \n",
       "298  روس نے ملک کی آبادی میں اضافے کیلئے خصوصی وزا...          world  \n",
       "299  ریاض: سعودی ولی عہد شہزادہ محمد بن سلمان نے کہ...          world  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc54f6b3-acd5-40ac-a586-cb9cbf101ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data has been successfully saved to separate CSV files: 'geo_scraped_data.csv', 'express_scraped_data.csv', and 'jang_scraped_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "geo_df.to_csv(\"geo_scraped_data.csv\", index=False)\n",
    "express_df.to_csv(\"express_scraped_data.csv\", index=False)\n",
    "jang_df.to_csv(\"jang_scraped_data.csv\", index=False)\n",
    "articles_df.to_csv(\"ary_scraped_data.csv\",index=False)\n",
    "dunya_df.to_csv(\"dunya_scraped_data.csv\",index = False)\n",
    "print(\"\\nData has been successfully saved to separate CSV files: 'geo_scraped_data.csv','dunya_scraped_data', 'express_scraped_data.csv','ary_scraped_data' and 'jang_scraped_data.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
